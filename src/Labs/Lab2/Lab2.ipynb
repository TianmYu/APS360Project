{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Maxwell Zheng (1004907871)\n",
    "Google Colab Link: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_indices(dataset, classes, target_classes):\n",
    "    \n",
    "    indices = []\n",
    "    for i in range(len(dataset)):\n",
    "        label_index = dataset[i][1]\n",
    "        label_class = classes[label_index]\n",
    "        if label_class in target_classes:\n",
    "            indices.append(i)\n",
    "            \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(target_classes, batch_size):\n",
    "    \n",
    "    classes = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n",
    "    \n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    relevant_indices = get_relevant_indices(trainset, classes, target_classes)\n",
    "    \n",
    "    np.random.seed(1000)\n",
    "    np.random.shuffle(relevant_indices)\n",
    "    split = int(len(relevant_indices) * 0.8)\n",
    "    \n",
    "    relevant_train_indices, relevant_val_indices = relevant_indices[:split], relevant_indices[split:]\n",
    "    train_sampler = SubsetRandomSampler(relevant_train_indices)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, num_workers=1, sampler=train_sampler)\n",
    "    val_sampler = SubsetRandomSampler(relevant_val_indices)\n",
    "    val_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, num_workers=1, sampler=val_sampler)\n",
    "    \n",
    "    testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "    relevant_test_indices = get_relevant_indices(testset, classes, target_classes)\n",
    "    test_sampler = SubsetRandomSampler(relevant_test_indices)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, num_workers=1, sampler=test_sampler)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    \n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name, batch_size, learning_rate, epoch)\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(labels):\n",
    "    \n",
    "    max_val = torch.max(labels)\n",
    "    min_val = torch.min(labels)\n",
    "    norm_labels = (labels - min_val)/(max_val - min_val)\n",
    "    \n",
    "    return norm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, loader, criterion):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_err = 0.0\n",
    "    total_epoch = 0\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        inputs, labels = data\n",
    "        labels = normalize_label(labels)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        corr = (outputs > 0.0).squeeze().long() != labels\n",
    "        total_err += int(corr.sum())\n",
    "        total_epoch += len(labels)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    err = float(total_err) / total_epoch\n",
    "    loss = float(total_loss) / (i + 1)\n",
    "    \n",
    "    return err, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curve(path):\n",
    "    \n",
    "    import matplotlib.plyplot as plt\n",
    "    \n",
    "    train_err = np.loadtxt(\"{}_train_err.csv\".format(path))\n",
    "    val_err = np.loadtxt(\"{}_val_err.csv\".format(path))\n",
    "    train_loss = np.loadtext(\"{}_train_loss.csv\".format(path))\n",
    "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
    "    plt.title(\"Train as Validation Error\")\n",
    "    n = len(train_err)\n",
    "    \n",
    "    plt.plot(range(1,n+1), train_err, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val_err, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title(\"Train vs Validation Loss\")\n",
    "    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, classes = get_data_loader(target_classes=[\"cat\", \"dog\"], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k = 0\n",
    "for images, labels in train_loader:\n",
    "    image = images[0]\n",
    "    img = np.transpose(image, [1,2,0])\n",
    "    img = img / 2 + 0.5\n",
    "    \n",
    "    plt.subplot(3,5,k+1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    k += 1\n",
    "    if k > 14:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1b\n",
    "\n",
    "\"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1c\n",
    "\n",
    "\"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LargeNet, self).__init__()\n",
    "        self.name = \"large\"\n",
    "        self.conv1 = nn.Conv2d(3, 5, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(5, 10, 5)\n",
    "        self.fc1 = nn.Linear(10*5*5, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 10*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.squeeze(1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SmallNet, self).__init__()\n",
    "        self.name = \"small\"\n",
    "        self.conv = nn.Conv2d(3, 5, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(5*7*7, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 5*7*7)\n",
    "        x = self.fc(x)\n",
    "        x = x.squeeze(1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_net = SmallNet()\n",
    "large_net = LargeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, batch_size=64, learning_rate=0.01, num_epochs=30):\n",
    "    \n",
    "    target_classes = [\"cat\", \"dog\"]\n",
    "    \n",
    "    torch.manual_seed(1000)\n",
    "    \n",
    "    train_loader, val_loader, test_loader, classes = get_data_loader(target_classes, batch_size)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    \n",
    "    train_err = np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_err = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0.0\n",
    "        total_train_err = 0.0\n",
    "        total_epoch = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            labels = normalize_label(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            corr = (outputs > 0.0).squeeze().long() != labels\n",
    "            total_train_err += int(corr.sum())\n",
    "            total_train_loss += loss.item()\n",
    "            total_epoch += len(labels)\n",
    "        train_err[epoch] = float(total_train_err) / total_epoch\n",
    "        train_loss[epoch] = float(total_train_loss) / total_epoch\n",
    "        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
    "        \n",
    "        print((\"Epoch {}: Train err: {}, Train loss: {} | Validation err: {}, Validation loss: {}\").format(epoch+1, train_err[epoch], train_loss[epoch], val_err[epoch], val_loss[epoch]))\n",
    "        \n",
    "        model_path = get_model_name(net.name, batch_size, learning_rate, epoch)\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "    print(\"Finished Training\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
    "    \n",
    "    epochs = np.arange(1, num_epochs+1)\n",
    "    \n",
    "    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
    "    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small\n",
      "torch.Size([5, 3, 3, 3])\n",
      "torch.Size([5])\n",
      "torch.Size([1, 245])\n",
      "torch.Size([1])\n",
      "\n",
      "Large\n",
      "torch.Size([5, 3, 5, 5])\n",
      "torch.Size([5])\n",
      "torch.Size([10, 5, 5, 5])\n",
      "torch.Size([10])\n",
      "torch.Size([32, 250])\n",
      "torch.Size([32])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Small\")\n",
    "for param in small_net.parameters():\n",
    "    print(param.shape)\n",
    "    \n",
    "print(\"\\nLarge\")\n",
    "for param in large_net.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a\n",
    "\n",
    "\"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b\n",
    "\n",
    "\"\"\"\n",
    "Default values:\n",
    "batch_size: 64\n",
    "leaning_rate: 0.01\n",
    "num_epochs: 30\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c\n",
    "\"\"\"\n",
    "Files written to disk:\n",
    "\n",
    "1. model_small_bs64_lr0.01_epoch4_train_err.csv\n",
    "    Info:\n",
    "    4.359999999999999987e-01\n",
    "    3.738750000000000129e-01\n",
    "    3.578749999999999987e-01\n",
    "    3.483749999999999902e-01\n",
    "    3.385000000000000231e-01\n",
    "2. model_small_bs64_lr0.01_epoch4_train_loss.csv\n",
    "    Info:\n",
    "    1.058855156600475335e-02\n",
    "    1.014952868223190259e-02\n",
    "    9.933517850935458596e-03\n",
    "    9.757397778332232985e-03\n",
    "    9.608423233032226812e-03\n",
    "3. model_small_bs64_lr0.01_epoch4_val_err.csv\n",
    "    Info\n",
    "    3.805000000000000049e-01\n",
    "    3.584999999999999853e-01\n",
    "    3.524999999999999800e-01\n",
    "    3.599999999999999867e-01\n",
    "    3.370000000000000218e-01\n",
    "4. model_small_bs64_lr0.01_epoch4_val_loss.csv\n",
    "    Info:\n",
    "    6.595180425792932510e-01\n",
    "    6.528661046177148819e-01\n",
    "    6.263249684125185013e-01\n",
    "    6.251449063420295715e-01\n",
    "    6.168369576334953308e-01\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d\n",
    "\"\"\"\n",
    "Small net: 347s\n",
    "Large net: 394s\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1: Train err: 0.436, Train loss: 0.010588551566004753 | Validation err: 0.3805, Validation loss: 0.6595180425792933\n",
      "Epoch 2: Train err: 0.373875, Train loss: 0.010149528682231903 | Validation err: 0.3585, Validation loss: 0.6528661046177149\n",
      "Epoch 3: Train err: 0.357875, Train loss: 0.009933517850935459 | Validation err: 0.3525, Validation loss: 0.6263249684125185\n",
      "Epoch 4: Train err: 0.348375, Train loss: 0.009757397778332233 | Validation err: 0.36, Validation loss: 0.6251449063420296\n",
      "Epoch 5: Train err: 0.3385, Train loss: 0.009608423233032227 | Validation err: 0.337, Validation loss: 0.6168369576334953\n",
      "Finished Training\n",
      "Total time elapsed: 57.48 seconds\n"
     ]
    }
   ],
   "source": [
    "train_net(small_net, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
